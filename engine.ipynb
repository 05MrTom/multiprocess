{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "trt_inference_async_v3.py\n",
    "\n",
    "A standalone module to load a TensorRT .engine file and run asynchronous inference using execute_async_v3.\n",
    "Usage:\n",
    "    python trt_inference_async_v3.py path/to/model.engine --image path/to/image.jpg --input_shape 1 3 640 640 --async_v3\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# Configure logger\n",
    "LOGGER = logging.getLogger(\"TensorRTInference\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Dummy helper functions. Modify as needed.\n",
    "def check_requirements(package):\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "def check_version(actual_version, version_constraint, hard=False, msg=\"\"):\n",
    "    # Implement version checking if required\n",
    "    pass\n",
    "\n",
    "# Environment flags; adjust these based on your setup.\n",
    "IS_JETSON = False  # Set True if running on a Jetson board\n",
    "LINUX = True       # Set True if running on Linux\n",
    "PYTHON_VERSION = \"3.8.10\"  # Adjust as needed\n",
    "\n",
    "# Updated binding structure to include a binding index.\n",
    "Binding = namedtuple(\"Binding\", (\"name\", \"idx\", \"dtype\", \"shape\", \"data\", \"ptr\"))\n",
    "\n",
    "class TensorRTInferenceEngine:\n",
    "    def __init__(self, engine_path, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the TensorRT inference engine.\n",
    "        :param engine_path: Path to the .engine file.\n",
    "        :param device: Torch device to run inference (defaults to CUDA if available).\n",
    "        \"\"\"\n",
    "        self.engine_path = engine_path\n",
    "        self.device = device or (torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.logger = trt.Logger(trt.Logger.INFO)\n",
    "        self.engine = None\n",
    "        self.context = None\n",
    "        self.bindings = OrderedDict()  # Mapping from binding name to Binding tuple.\n",
    "        self.output_names = []\n",
    "        self.dynamic = False\n",
    "        self.fp16 = False\n",
    "        self.is_trt10 = None  # Will be determined during engine loading.\n",
    "        self._load_engine()\n",
    "\n",
    "    def _load_engine(self):\n",
    "        \"\"\"Load the engine file, deserialize it, and set up the bindings.\"\"\"\n",
    "        with open(self.engine_path, \"rb\") as f, trt.Runtime(self.logger) as runtime:\n",
    "            # Try to read metadata (if present)\n",
    "            try:\n",
    "                meta_len = int.from_bytes(f.read(4), byteorder=\"little\")\n",
    "                metadata = json.loads(f.read(meta_len).decode(\"utf-8\"))\n",
    "            except Exception:\n",
    "                LOGGER.info(\"No metadata found in engine file. Proceeding without metadata.\")\n",
    "                f.seek(0)\n",
    "                metadata = {}\n",
    "\n",
    "            # Set DLA core if provided in metadata\n",
    "            dla = metadata.get(\"dla\", None)\n",
    "            if dla is not None:\n",
    "                runtime.DLA_core = int(dla)\n",
    "\n",
    "            # Deserialize engine\n",
    "            engine_data = f.read()\n",
    "            self.engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "        # Create execution context\n",
    "        try:\n",
    "            self.context = self.engine.create_execution_context()\n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"Error creating execution context: {e}\")\n",
    "            raise e\n",
    "\n",
    "        # Determine if using TRT 10+ API.\n",
    "        # If the engine does not have \"num_bindings\", assume TRT 10+ API.\n",
    "        self.is_trt10 = not hasattr(self.engine, \"num_bindings\")\n",
    "        num_tensors = self.engine.num_io_tensors if self.is_trt10 else self.engine.num_bindings\n",
    "\n",
    "        for i in range(num_tensors):\n",
    "            if self.is_trt10:\n",
    "                # For TRT 10+ API\n",
    "                name = self.engine.get_tensor_name(i)\n",
    "                dtype = trt.nptype(self.engine.get_tensor_dtype(name))\n",
    "                is_input = self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT\n",
    "                try:\n",
    "                    idx = self.engine.get_binding_index(name)\n",
    "                except AttributeError:\n",
    "                    idx = i\n",
    "                if is_input:\n",
    "                    shape = tuple(self.engine.get_tensor_shape(name))\n",
    "                    # Check for dynamic shape (indicated by -1)\n",
    "                    if -1 in shape:\n",
    "                        self.dynamic = True\n",
    "                        default_shape = tuple(self.engine.get_tensor_profile_shape(name, 0)[1])\n",
    "                        self.context.set_input_shape(name, default_shape)\n",
    "                        shape = default_shape\n",
    "                    if dtype == np.float16:\n",
    "                        self.fp16 = True\n",
    "                else:\n",
    "                    self.output_names.append(name)\n",
    "                    shape = tuple(self.context.get_tensor_shape(name))\n",
    "            else:\n",
    "                # For pre-TRT 10 API\n",
    "                name = self.engine.get_binding_name(i)\n",
    "                dtype = trt.nptype(self.engine.get_binding_dtype(i))\n",
    "                is_input = self.engine.binding_is_input(i)\n",
    "                idx = i  # Use loop index since get_binding_index is not available.\n",
    "                if is_input:\n",
    "                    shape = tuple(self.engine.get_binding_shape(i))\n",
    "                    if -1 in shape:\n",
    "                        self.dynamic = True\n",
    "                        default_shape = tuple(self.engine.get_profile_shape(0, i)[1])\n",
    "                        self.context.set_binding_shape(i, default_shape)\n",
    "                        shape = default_shape\n",
    "                    if dtype == np.float16:\n",
    "                        self.fp16 = True\n",
    "                else:\n",
    "                    self.output_names.append(name)\n",
    "                    shape = tuple(self.context.get_tensor_shape(i))\n",
    "            # Allocate buffer using a PyTorch tensor\n",
    "            tensor = torch.from_numpy(np.empty(shape, dtype=dtype)).to(self.device)\n",
    "            self.bindings[name] = Binding(name, idx, dtype, shape, tensor, int(tensor.data_ptr()))\n",
    "        LOGGER.info(\"Engine loaded and bindings initialized.\")\n",
    "\n",
    "    def infer(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Run synchronous inference on the input tensor.\n",
    "        :param input_tensor: A torch.Tensor with shape matching the model's expected input.\n",
    "        :return: List of output tensors.\n",
    "        \"\"\"\n",
    "        input_binding = self.bindings[\"images\"]\n",
    "        if self.dynamic and input_tensor.shape != input_binding.shape:\n",
    "            if self.is_trt10:\n",
    "                self.context.set_input_shape(\"images\", input_tensor.shape)\n",
    "                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=input_tensor.shape)\n",
    "                for name in self.output_names:\n",
    "                    new_shape = tuple(self.context.get_tensor_shape(name))\n",
    "                    self.bindings[name].data.resize_(new_shape)\n",
    "            else:\n",
    "                idx = self.bindings[\"images\"].idx\n",
    "                self.context.set_binding_shape(idx, input_tensor.shape)\n",
    "                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=input_tensor.shape)\n",
    "                for name in self.output_names:\n",
    "                    idx = self.bindings[name].idx\n",
    "                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(idx)))\n",
    "        expected_shape = self.bindings[\"images\"].shape\n",
    "        assert input_tensor.shape == expected_shape, (\n",
    "            f\"Input size {input_tensor.shape} does not match expected shape {expected_shape}\"\n",
    "        )\n",
    "        # For synchronous inference, update input tensor address.\n",
    "        self.context.set_tensor_address(\"images\", int(input_tensor.data_ptr()))\n",
    "        self.context.execute_v2([b.ptr for b in self.bindings.values()])\n",
    "        outputs = [self.bindings[name].data for name in sorted(self.output_names)]\n",
    "        return outputs\n",
    "\n",
    "    def infer_async_v3(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Run asynchronous inference on the input tensor using execute_async_v3.\n",
    "        This requires a TensorRT version that supports execute_async_v3.\n",
    "        :param input_tensor: A torch.Tensor with shape matching the model's expected input.\n",
    "        :return: List of output tensors.\n",
    "        \"\"\"\n",
    "        if not input_tensor.is_cuda:\n",
    "            input_tensor = input_tensor.to(self.device)\n",
    "\n",
    "        input_binding = self.bindings[\"images\"]\n",
    "        if self.dynamic and input_tensor.shape != input_binding.shape:\n",
    "            if self.is_trt10:\n",
    "                self.context.set_input_shape(\"images\", input_tensor.shape)\n",
    "                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=input_tensor.shape)\n",
    "                for name in self.output_names:\n",
    "                    new_shape = tuple(self.context.get_tensor_shape(name))\n",
    "                    self.bindings[name].data.resize_(new_shape)\n",
    "            else:\n",
    "                idx = self.bindings[\"images\"].idx\n",
    "                self.context.set_binding_shape(idx, input_tensor.shape)\n",
    "                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=input_tensor.shape)\n",
    "                for name in self.output_names:\n",
    "                    idx = self.bindings[name].idx\n",
    "                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(idx)))\n",
    "        expected_shape = self.bindings[\"images\"].shape\n",
    "        assert input_tensor.shape == expected_shape, (\n",
    "            f\"Input size {input_tensor.shape} does not match expected shape {expected_shape}\"\n",
    "        )\n",
    "\n",
    "        # Update the tensor addresses using set_tensor_address and the stored tensor names.\n",
    "        for name, binding in self.bindings.items():\n",
    "            if name == \"images\":\n",
    "                # For input, update the address from the provided tensor.\n",
    "                self.context.set_tensor_address(name, int(input_tensor.data_ptr()))\n",
    "            else:\n",
    "                # For outputs, use the stored pointer.\n",
    "                self.context.set_tensor_address(name, binding.ptr)\n",
    "\n",
    "        stream = torch.cuda.Stream() if self.device.type == \"cuda\" else None\n",
    "        if stream is None:\n",
    "            LOGGER.warning(\"No CUDA stream available. Falling back to synchronous inference.\")\n",
    "            return self.infer(input_tensor)\n",
    "\n",
    "        if not hasattr(self.context, \"execute_async_v3\"):\n",
    "            LOGGER.warning(\"execute_async_v3 not supported. Falling back to execute_async_v2.\")\n",
    "            return self.infer_async(input_tensor)\n",
    "\n",
    "        self.context.execute_async_v3(stream.cuda_stream)\n",
    "        stream.synchronize()\n",
    "        outputs = [self.bindings[name].data for name in sorted(self.output_names)]\n",
    "        return outputs\n",
    "\n",
    "    def infer_async(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Run asynchronous inference on the input tensor using execute_async_v2.\n",
    "        This is kept for comparison.\n",
    "        :param input_tensor: A torch.Tensor with shape matching the model's expected input.\n",
    "        :return: List of output tensors.\n",
    "        \"\"\"\n",
    "        if not input_tensor.is_cuda:\n",
    "            input_tensor = input_tensor.to(self.device)\n",
    "\n",
    "        input_binding = self.bindings[\"images\"]\n",
    "        if self.dynamic and input_tensor.shape != input_binding.shape:\n",
    "            if self.is_trt10:\n",
    "                self.context.set_input_shape(\"images\", input_tensor.shape)\n",
    "                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=input_tensor.shape)\n",
    "                for name in self.output_names:\n",
    "                    new_shape = tuple(self.context.get_tensor_shape(name))\n",
    "                    self.bindings[name].data.resize_(new_shape)\n",
    "            else:\n",
    "                idx = self.bindings[\"images\"].idx\n",
    "                self.context.set_binding_shape(idx, input_tensor.shape)\n",
    "                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=input_tensor.shape)\n",
    "                for name in self.output_names:\n",
    "                    idx = self.bindings[name].idx\n",
    "                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(idx)))\n",
    "        expected_shape = self.bindings[\"images\"].shape\n",
    "        assert input_tensor.shape == expected_shape, (\n",
    "            f\"Input size {input_tensor.shape} does not match expected shape {expected_shape}\"\n",
    "        )\n",
    "        self.context.set_tensor_address(\"images\", int(input_tensor.data_ptr()))\n",
    "        stream = torch.cuda.Stream() if self.device.type == \"cuda\" else None\n",
    "        if stream is None:\n",
    "            LOGGER.warning(\"No CUDA stream available. Falling back to synchronous inference.\")\n",
    "            return self.infer(input_tensor)\n",
    "        self.context.execute_async_v2([b.ptr for b in self.bindings.values()], stream.cuda_stream)\n",
    "        stream.synchronize()\n",
    "        y = [self.bindings[name].data for name in sorted(self.output_names)]\n",
    "        \n",
    "        if isinstance(y, (list, tuple)):\n",
    "            if len(self.names) == 999 and (self.task == \"segment\" or len(y) == 2):  # segments and names not defined\n",
    "                nc = y[0].shape[1] - y[1].shape[1] - 4  # y = (1, 32, 160, 160), (1, 116, 8400)\n",
    "                self.names = {i: f\"class{i}\" for i in range(nc)}\n",
    "            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\n",
    "        else:\n",
    "            return self.from_numpy(y)\n",
    "    \n",
    "    def from_numpy(self, x):\n",
    "        \"\"\"\n",
    "        Convert a numpy array to a tensor.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): The array to be converted.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): The converted tensor\n",
    "        \"\"\"\n",
    "        return torch.tensor(x).to(self.device) if isinstance(x, np.ndarray) else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    Takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): The bounding boxes to clip.\n",
    "        shape (tuple): The shape of the image.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor | numpy.ndarray): The clipped boxes.\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)\n",
    "        boxes[..., 0] = boxes[..., 0].clamp(0, shape[1])  # x1\n",
    "        boxes[..., 1] = boxes[..., 1].clamp(0, shape[0])  # y1\n",
    "        boxes[..., 2] = boxes[..., 2].clamp(0, shape[1])  # x2\n",
    "        boxes[..., 3] = boxes[..., 3].clamp(0, shape[0])  # y2\n",
    "    else:  # np.array (faster grouped)\n",
    "        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True, xywh=False):\n",
    "    \"\"\"\n",
    "    Rescales bounding boxes (in the format of xyxy by default) from the shape of the image they were originally\n",
    "    specified in (img1_shape) to the shape of a different image (img0_shape).\n",
    "\n",
    "    Args:\n",
    "        img1_shape (tuple): The shape of the image that the bounding boxes are for, in the format of (height, width).\n",
    "        boxes (torch.Tensor): the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)\n",
    "        img0_shape (tuple): the shape of the target image, in the format of (height, width).\n",
    "        ratio_pad (tuple): a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be\n",
    "            calculated based on the size difference between the two images.\n",
    "        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n",
    "            rescaling.\n",
    "        xywh (bool): The box format is xywh or not, default=False.\n",
    "\n",
    "    Returns:\n",
    "        boxes (torch.Tensor): The scaled bounding boxes, in the format of (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (\n",
    "            round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1),\n",
    "            round((img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1),\n",
    "        )  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    if padding:\n",
    "        boxes[..., 0] -= pad[0]  # x padding\n",
    "        boxes[..., 1] -= pad[1]  # y padding\n",
    "        if not xywh:\n",
    "            boxes[..., 2] -= pad[0]  # x padding\n",
    "            boxes[..., 3] -= pad[1]  # y padding\n",
    "    boxes[..., :4] /= gain\n",
    "    return clip_boxes(boxes, img0_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def scale_image(masks, im0_shape, ratio_pad=None):\n",
    "    \"\"\"\n",
    "    Takes a mask, and resizes it to the original image size.\n",
    "\n",
    "    Args:\n",
    "        masks (np.ndarray): Resized and padded masks/images, [h, w, num]/[h, w, 3].\n",
    "        im0_shape (tuple): The original image shape.\n",
    "        ratio_pad (tuple): The ratio of the padding to the original image.\n",
    "\n",
    "    Returns:\n",
    "        masks (np.ndarray): The masks that are being returned with shape [h, w, num].\n",
    "    \"\"\"\n",
    "    # Rescale coordinates (xyxy) from im1_shape to im0_shape\n",
    "    im1_shape = masks.shape\n",
    "    if im1_shape[:2] == im0_shape[:2]:\n",
    "        return masks\n",
    "    if ratio_pad is None:  # calculate from im0_shape\n",
    "        gain = min(im1_shape[0] / im0_shape[0], im1_shape[1] / im0_shape[1])  # gain  = old / new\n",
    "        pad = (im1_shape[1] - im0_shape[1] * gain) / 2, (im1_shape[0] - im0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        # gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "    top, left = int(pad[1]), int(pad[0])  # y, x\n",
    "    bottom, right = int(im1_shape[0] - pad[1]), int(im1_shape[1] - pad[0])\n",
    "\n",
    "    if len(masks.shape) < 2:\n",
    "        raise ValueError(f'\"len of masks shape\" should be 2 or 3, but got {len(masks.shape)}')\n",
    "    masks = masks[top:bottom, left:right]\n",
    "    masks = cv2.resize(masks, (im0_shape[1], im0_shape[0]))\n",
    "    if len(masks.shape) == 2:\n",
    "        masks = masks[:, :, None]\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def empty_like(x):\n",
    "    \"\"\"Creates empty torch.Tensor or np.ndarray with same shape as input and float32 dtype.\"\"\"\n",
    "    return (\n",
    "        torch.empty_like(x, dtype=torch.float32) if isinstance(x, torch.Tensor) else np.empty_like(x, dtype=np.float32)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner. Note: ops per 2 channels faster than per channel.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
    "    y = empty_like(x)  # faster than clone/copy\n",
    "    xy = x[..., :2]  # centers\n",
    "    wh = x[..., 2:] / 2  # half width-height\n",
    "    y[..., :2] = xy - wh  # top left xy\n",
    "    y[..., 2:] = xy + wh  # bottom right xy\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def nms_rotated(boxes, scores, threshold=0.45, use_triu=True):\n",
    "    \"\"\"\n",
    "    NMS for oriented bounding boxes using probiou and fast-nms.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): Rotated bounding boxes, shape (N, 5), format xywhr.\n",
    "        scores (torch.Tensor): Confidence scores, shape (N,).\n",
    "        threshold (float, optional): IoU threshold. Defaults to 0.45.\n",
    "        use_triu (bool, optional): Whether to use `torch.triu` operator. It'd be useful for disable it\n",
    "            when exporting obb models to some formats that do not support `torch.triu`.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Indices of boxes to keep after NMS.\n",
    "    \"\"\"\n",
    "    sorted_idx = torch.argsort(scores, descending=True)\n",
    "    boxes = boxes[sorted_idx]\n",
    "    ious = batch_probiou(boxes, boxes)\n",
    "    if use_triu:\n",
    "        ious = ious.triu_(diagonal=1)\n",
    "        # pick = torch.nonzero(ious.max(dim=0)[0] < threshold).squeeze_(-1)\n",
    "        # NOTE: handle the case when len(boxes) hence exportable by eliminating if-else condition\n",
    "        pick = torch.nonzero((ious >= threshold).sum(0) <= 0).squeeze_(-1)\n",
    "    else:\n",
    "        n = boxes.shape[0]\n",
    "        row_idx = torch.arange(n, device=boxes.device).view(-1, 1).expand(-1, n)\n",
    "        col_idx = torch.arange(n, device=boxes.device).view(1, -1).expand(n, -1)\n",
    "        upper_mask = row_idx < col_idx\n",
    "        ious = ious * upper_mask\n",
    "        # Zeroing these scores ensures the additional indices would not affect the final results\n",
    "        scores[~((ious >= threshold).sum(0) <= 0)] = 0\n",
    "        # NOTE: return indices with fixed length to avoid TFLite reshape error\n",
    "        pick = torch.topk(scores, scores.shape[0]).indices\n",
    "    return sorted_idx[pick]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def non_max_suppression(\n",
    "    prediction,\n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.45,\n",
    "    classes=None,\n",
    "    agnostic=False,\n",
    "    multi_label=False,\n",
    "    labels=(),\n",
    "    max_det=300,\n",
    "    nc=0,  # number of classes (optional)\n",
    "    max_time_img=0.05,\n",
    "    max_nms=30000,\n",
    "    max_wh=7680,\n",
    "    in_place=True,\n",
    "    rotated=False,\n",
    "    end2end=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.\n",
    "\n",
    "    Args:\n",
    "        prediction (torch.Tensor): A tensor of shape (batch_size, num_classes + 4 + num_masks, num_boxes)\n",
    "            containing the predicted boxes, classes, and masks. The tensor should be in the format\n",
    "            output by a model, such as YOLO.\n",
    "        conf_thres (float): The confidence threshold below which boxes will be filtered out.\n",
    "            Valid values are between 0.0 and 1.0.\n",
    "        iou_thres (float): The IoU threshold below which boxes will be filtered out during NMS.\n",
    "            Valid values are between 0.0 and 1.0.\n",
    "        classes (List[int]): A list of class indices to consider. If None, all classes will be considered.\n",
    "        agnostic (bool): If True, the model is agnostic to the number of classes, and all\n",
    "            classes will be considered as one.\n",
    "        multi_label (bool): If True, each box may have multiple labels.\n",
    "        labels (List[List[Union[int, float, torch.Tensor]]]): A list of lists, where each inner\n",
    "            list contains the apriori labels for a given image. The list should be in the format\n",
    "            output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).\n",
    "        max_det (int): The maximum number of boxes to keep after NMS.\n",
    "        nc (int, optional): The number of classes output by the model. Any indices after this will be considered masks.\n",
    "        max_time_img (float): The maximum time (seconds) for processing one image.\n",
    "        max_nms (int): The maximum number of boxes into torchvision.ops.nms().\n",
    "        max_wh (int): The maximum box width and height in pixels.\n",
    "        in_place (bool): If True, the input prediction tensor will be modified in place.\n",
    "        rotated (bool): If Oriented Bounding Boxes (OBB) are being passed for NMS.\n",
    "        end2end (bool): If the model doesn't require NMS.\n",
    "\n",
    "    Returns:\n",
    "        (List[torch.Tensor]): A list of length batch_size, where each element is a tensor of\n",
    "            shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns\n",
    "            (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).\n",
    "    \"\"\"\n",
    "    import torchvision  # scope for faster 'import ultralytics'\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
    "    assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
    "    if isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)\n",
    "        prediction = prediction[0]  # select only inference output\n",
    "    if classes is not None:\n",
    "        classes = torch.tensor(classes, device=prediction.device)\n",
    "\n",
    "    if prediction.shape[-1] == 6 or end2end:  # end-to-end model (BNC, i.e. 1,300,6)\n",
    "        output = [pred[pred[:, 4] > conf_thres][:max_det] for pred in prediction]\n",
    "        if classes is not None:\n",
    "            output = [pred[(pred[:, 5:6] == classes).any(1)] for pred in output]\n",
    "        return output\n",
    "\n",
    "    bs = prediction.shape[0]  # batch size (BCN, i.e. 1,84,6300)\n",
    "    nc = nc or (prediction.shape[1] - 4)  # number of classes\n",
    "    nm = prediction.shape[1] - nc - 4  # number of masks\n",
    "    mi = 4 + nc  # mask start index\n",
    "    xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    time_limit = 2.0 + max_time_img * bs  # seconds to quit after\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "    prediction = prediction.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)\n",
    "    if not rotated:\n",
    "        if in_place:\n",
    "            prediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy\n",
    "        else:\n",
    "            prediction = torch.cat((xywh2xyxy(prediction[..., :4]), prediction[..., 4:]), dim=-1)  # xywh to xyxy\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]) and not rotated:\n",
    "            lb = labels[xi]\n",
    "            v = torch.zeros((len(lb), nc + nm + 4), device=x.device)\n",
    "            v[:, :4] = xywh2xyxy(lb[:, 1:5])  # box\n",
    "            v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        box, cls, mask = x.split((4, nc, nm), 1)\n",
    "\n",
    "        if multi_label:\n",
    "            i, j = torch.where(cls > conf_thres)\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == classes).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        if n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        scores = x[:, 4]  # scores\n",
    "        if rotated:\n",
    "            boxes = torch.cat((x[:, :2] + c, x[:, 2:4], x[:, -1:]), dim=-1)  # xywhr\n",
    "            i = nms_rotated(boxes, scores, iou_thres)\n",
    "        else:\n",
    "            boxes = x[:, :4] + c  # boxes (offset by class)\n",
    "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "\n",
    "        # # Experimental\n",
    "        # merge = False  # use merge-NMS\n",
    "        # if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "        #     # Update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "        #     from .metrics import box_iou\n",
    "        #     iou = box_iou(boxes[i], boxes) > iou_thres  # IoU matrix\n",
    "        #     weights = iou * scores[None]  # box weights\n",
    "        #     x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "        #     redundant = True  # require redundant detections\n",
    "        #     if redundant:\n",
    "        #         i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            LOGGER.warning(f\"WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded\")\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _get_covariance_matrix(boxes):\n",
    "    \"\"\"\n",
    "    Generating covariance matrix from obbs.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): A tensor of shape (N, 5) representing rotated bounding boxes, with xywhr format.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Covariance matrices corresponding to original rotated bounding boxes.\n",
    "    \"\"\"\n",
    "    # Gaussian bounding boxes, ignore the center points (the first two columns) because they are not needed here.\n",
    "    gbbs = torch.cat((boxes[:, 2:4].pow(2) / 12, boxes[:, 4:]), dim=-1)\n",
    "    a, b, c = gbbs.split(1, dim=-1)\n",
    "    cos = c.cos()\n",
    "    sin = c.sin()\n",
    "    cos2 = cos.pow(2)\n",
    "    sin2 = sin.pow(2)\n",
    "    return a * cos2 + b * sin2, a * sin2 + b * cos2, (a - b) * cos * sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def batch_probiou(obb1, obb2, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate the prob IoU between oriented bounding boxes, https://arxiv.org/pdf/2106.06072v1.pdf.\n",
    "\n",
    "    Args:\n",
    "        obb1 (torch.Tensor | np.ndarray): A tensor of shape (N, 5) representing ground truth obbs, with xywhr format.\n",
    "        obb2 (torch.Tensor | np.ndarray): A tensor of shape (M, 5) representing predicted obbs, with xywhr format.\n",
    "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): A tensor of shape (N, M) representing obb similarities.\n",
    "    \"\"\"\n",
    "    obb1 = torch.from_numpy(obb1) if isinstance(obb1, np.ndarray) else obb1\n",
    "    obb2 = torch.from_numpy(obb2) if isinstance(obb2, np.ndarray) else obb2\n",
    "\n",
    "    x1, y1 = obb1[..., :2].split(1, dim=-1)\n",
    "    x2, y2 = (x.squeeze(-1)[None] for x in obb2[..., :2].split(1, dim=-1))\n",
    "    a1, b1, c1 = _get_covariance_matrix(obb1)\n",
    "    a2, b2, c2 = (x.squeeze(-1)[None] for x in _get_covariance_matrix(obb2))\n",
    "\n",
    "    t1 = (\n",
    "        ((a1 + a2) * (y1 - y2).pow(2) + (b1 + b2) * (x1 - x2).pow(2)) / ((a1 + a2) * (b1 + b2) - (c1 + c2).pow(2) + eps)\n",
    "    ) * 0.25\n",
    "    t2 = (((c1 + c2) * (x2 - x1) * (y1 - y2)) / ((a1 + a2) * (b1 + b2) - (c1 + c2).pow(2) + eps)) * 0.5\n",
    "    t3 = (\n",
    "        ((a1 + a2) * (b1 + b2) - (c1 + c2).pow(2))\n",
    "        / (4 * ((a1 * b1 - c1.pow(2)).clamp_(0) * (a2 * b2 - c2.pow(2)).clamp_(0)).sqrt() + eps)\n",
    "        + eps\n",
    "    ).log() * 0.5\n",
    "    bd = (t1 + t2 + t3).clamp(eps, 100.0)\n",
    "    hd = (1.0 - (-bd).exp() + eps).sqrt()\n",
    "    return 1 - hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clip_coords(coords, shape):\n",
    "    \"\"\"\n",
    "    Clip line coordinates to the image boundaries.\n",
    "\n",
    "    Args:\n",
    "        coords (torch.Tensor | numpy.ndarray): A list of line coordinates.\n",
    "        shape (tuple): A tuple of integers representing the size of the image in the format (height, width).\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor | numpy.ndarray): Clipped coordinates\n",
    "    \"\"\"\n",
    "    if isinstance(coords, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)\n",
    "        coords[..., 0] = coords[..., 0].clamp(0, shape[1])  # x\n",
    "        coords[..., 1] = coords[..., 1].clamp(0, shape[0])  # y\n",
    "    else:  # np.array (faster grouped)\n",
    "        coords[..., 0] = coords[..., 0].clip(0, shape[1])  # x\n",
    "        coords[..., 1] = coords[..., 1].clip(0, shape[0])  # y\n",
    "    return coords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None, normalize=False, padding=True):\n",
    "    \"\"\"\n",
    "    Rescale segment coordinates (xy) from img1_shape to img0_shape.\n",
    "\n",
    "    Args:\n",
    "        img1_shape (tuple): The shape of the image that the coords are from.\n",
    "        coords (torch.Tensor): the coords to be scaled of shape n,2.\n",
    "        img0_shape (tuple): the shape of the image that the segmentation is being applied to.\n",
    "        ratio_pad (tuple): the ratio of the image size to the padded image size.\n",
    "        normalize (bool): If True, the coordinates will be normalized to the range [0, 1]. Defaults to False.\n",
    "        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n",
    "            rescaling.\n",
    "\n",
    "    Returns:\n",
    "        coords (torch.Tensor): The scaled coordinates.\n",
    "    \"\"\"\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    if padding:\n",
    "        coords[..., 0] -= pad[0]  # x padding\n",
    "        coords[..., 1] -= pad[1]  # y padding\n",
    "    coords[..., 0] /= gain\n",
    "    coords[..., 1] /= gain\n",
    "    coords = clip_coords(coords, img0_shape)\n",
    "    if normalize:\n",
    "        coords[..., 0] /= img0_shape[1]  # width\n",
    "        coords[..., 1] /= img0_shape[0]  # height\n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LetterBox:\n",
    "    \"\"\"\n",
    "    Resize image and padding for detection, instance segmentation, pose.\n",
    "\n",
    "    This class resizes and pads images to a specified shape while preserving aspect ratio. It also updates\n",
    "    corresponding labels and bounding boxes.\n",
    "\n",
    "    Attributes:\n",
    "        new_shape (tuple): Target shape (height, width) for resizing.\n",
    "        auto (bool): Whether to use minimum rectangle.\n",
    "        scale_fill (bool): Whether to stretch the image to new_shape.\n",
    "        scaleup (bool): Whether to allow scaling up. If False, only scale down.\n",
    "        stride (int): Stride for rounding padding.\n",
    "        center (bool): Whether to center the image or align to top-left.\n",
    "\n",
    "    Methods:\n",
    "        __call__: Resize and pad image, update labels and bounding boxes.\n",
    "\n",
    "    Examples:\n",
    "        >>> transform = LetterBox(new_shape=(640, 640))\n",
    "        >>> result = transform(labels)\n",
    "        >>> resized_img = result[\"img\"]\n",
    "        >>> updated_instances = result[\"instances\"]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, new_shape=(640, 640), auto=False, scale_fill=False, scaleup=True, center=True, stride=32):\n",
    "        \"\"\"\n",
    "        Initialize LetterBox object for resizing and padding images.\n",
    "\n",
    "        This class is designed to resize and pad images for object detection, instance segmentation, and pose estimation\n",
    "        tasks. It supports various resizing modes including auto-sizing, scale-fill, and letterboxing.\n",
    "\n",
    "        Args:\n",
    "            new_shape (Tuple[int, int]): Target size (height, width) for the resized image.\n",
    "            auto (bool): If True, use minimum rectangle to resize. If False, use new_shape directly.\n",
    "            scale_fill (bool): If True, stretch the image to new_shape without padding.\n",
    "            scaleup (bool): If True, allow scaling up. If False, only scale down.\n",
    "            center (bool): If True, center the placed image. If False, place image in top-left corner.\n",
    "            stride (int): Stride of the model (e.g., 32 for YOLOv5).\n",
    "\n",
    "        Attributes:\n",
    "            new_shape (Tuple[int, int]): Target size for the resized image.\n",
    "            auto (bool): Flag for using minimum rectangle resizing.\n",
    "            scale_fill (bool): Flag for stretching image without padding.\n",
    "            scaleup (bool): Flag for allowing upscaling.\n",
    "            stride (int): Stride value for ensuring image size is divisible by stride.\n",
    "\n",
    "        Examples:\n",
    "            >>> letterbox = LetterBox(new_shape=(640, 640), auto=False, scale_fill=False, scaleup=True, stride=32)\n",
    "            >>> resized_img = letterbox(original_img)\n",
    "        \"\"\"\n",
    "        self.new_shape = new_shape\n",
    "        self.auto = auto\n",
    "        self.scale_fill = scale_fill\n",
    "        self.scaleup = scaleup\n",
    "        self.stride = stride\n",
    "        self.center = center  # Put the image in the middle or top-left\n",
    "\n",
    "    def __call__(self, labels=None, image=None):\n",
    "        \"\"\"\n",
    "        Resizes and pads an image for object detection, instance segmentation, or pose estimation tasks.\n",
    "\n",
    "        This method applies letterboxing to the input image, which involves resizing the image while maintaining its\n",
    "        aspect ratio and adding padding to fit the new shape. It also updates any associated labels accordingly.\n",
    "\n",
    "        Args:\n",
    "            labels (Dict | None): A dictionary containing image data and associated labels, or empty dict if None.\n",
    "            image (np.ndarray | None): The input image as a numpy array. If None, the image is taken from 'labels'.\n",
    "\n",
    "        Returns:\n",
    "            (Dict | Tuple): If 'labels' is provided, returns an updated dictionary with the resized and padded image,\n",
    "                updated labels, and additional metadata. If 'labels' is empty, returns a tuple containing the resized\n",
    "                and padded image, and a tuple of (ratio, (left_pad, top_pad)).\n",
    "\n",
    "        Examples:\n",
    "            >>> letterbox = LetterBox(new_shape=(640, 640))\n",
    "            >>> result = letterbox(labels={\"img\": np.zeros((480, 640, 3)), \"instances\": Instances(...)})\n",
    "            >>> resized_img = result[\"img\"]\n",
    "            >>> updated_instances = result[\"instances\"]\n",
    "        \"\"\"\n",
    "        if labels is None:\n",
    "            labels = {}\n",
    "        img = labels.get(\"img\") if image is None else image\n",
    "        shape = img.shape[:2]  # current shape [height, width]\n",
    "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
    "        if isinstance(new_shape, int):\n",
    "            new_shape = (new_shape, new_shape)\n",
    "\n",
    "        # Scale ratio (new / old)\n",
    "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "        if not self.scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "            r = min(r, 1.0)\n",
    "\n",
    "        # Compute padding\n",
    "        ratio = r, r  # width, height ratios\n",
    "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "        if self.auto:  # minimum rectangle\n",
    "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
    "        elif self.scale_fill:  # stretch\n",
    "            dw, dh = 0.0, 0.0\n",
    "            new_unpad = (new_shape[1], new_shape[0])\n",
    "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "        if self.center:\n",
    "            dw /= 2  # divide padding into 2 sides\n",
    "            dh /= 2\n",
    "\n",
    "        if shape[::-1] != new_unpad:  # resize\n",
    "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
    "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
    "        img = cv2.copyMakeBorder(\n",
    "            img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114)\n",
    "        )  # add border\n",
    "        if labels.get(\"ratio_pad\"):\n",
    "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
    "\n",
    "        if len(labels):\n",
    "            labels = self._update_labels(labels, ratio, left, top)\n",
    "            labels[\"img\"] = img\n",
    "            labels[\"resized_shape\"] = new_shape\n",
    "            return labels\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_labels(labels, ratio, padw, padh):\n",
    "        \"\"\"\n",
    "        Updates labels after applying letterboxing to an image.\n",
    "\n",
    "        This method modifies the bounding box coordinates of instances in the labels\n",
    "        to account for resizing and padding applied during letterboxing.\n",
    "\n",
    "        Args:\n",
    "            labels (Dict): A dictionary containing image labels and instances.\n",
    "            ratio (Tuple[float, float]): Scaling ratios (width, height) applied to the image.\n",
    "            padw (float): Padding width added to the image.\n",
    "            padh (float): Padding height added to the image.\n",
    "\n",
    "        Returns:\n",
    "            (Dict): Updated labels dictionary with modified instance coordinates.\n",
    "\n",
    "        Examples:\n",
    "            >>> letterbox = LetterBox(new_shape=(640, 640))\n",
    "            >>> labels = {\"instances\": Instances(...)}\n",
    "            >>> ratio = (0.5, 0.5)\n",
    "            >>> padw, padh = 10, 20\n",
    "            >>> updated_labels = letterbox._update_labels(labels, ratio, padw, padh)\n",
    "        \"\"\"\n",
    "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
    "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
    "        labels[\"instances\"].scale(*ratio)\n",
    "        labels[\"instances\"].add_padding(padw, padh)\n",
    "        return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def pre_transform(im):\n",
    "    \"\"\"\n",
    "    Pre-transform input image before inference.\n",
    "\n",
    "    Args:\n",
    "        im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n",
    "\n",
    "    Returns:\n",
    "        (list): A list of transformed images.\n",
    "    \"\"\"\n",
    "    same_shapes = len({x.shape for x in im}) == 1\n",
    "    letterbox = LetterBox()\n",
    "    return [letterbox(image=x) for x in im]\n",
    "\n",
    "def preprocess(im):\n",
    "    \"\"\"\n",
    "    Prepares input image before inference.\n",
    "\n",
    "    Args:\n",
    "        im (torch.Tensor | List(np.ndarray)): BCHW for tensor, [(HWC) x B] for list.\n",
    "    \"\"\"\n",
    "    not_tensor = not isinstance(im, torch.Tensor)\n",
    "    if not_tensor:\n",
    "        im = np.stack(pre_transform(im))\n",
    "        im = im[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW, (n, 3, h, w)\n",
    "        im = np.ascontiguousarray(im)  # contiguous\n",
    "        im = torch.from_numpy(im)\n",
    "\n",
    "    im = im.to(device)\n",
    "    im = im.float()  # uint8 to fp16/32\n",
    "    if not_tensor:\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TensorRTInference:Engine loaded and bindings initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/11/2025-10:08:13] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[03/11/2025-10:08:13] [TRT] [I] Loaded engine size: 14 MiB\n",
      "[03/11/2025-10:08:13] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +20, now: CPU 1, GPU 65 (MiB)\n",
      "input_tensor shape torch.Size([1, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from ultralytics.utils.ops import non_max_suppression, scale_boxes, scale_coords\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "engine = TensorRTInferenceEngine(\"/home/amrit05/projects/shuttlengine/yolo11n-pose.engine\")\n",
    "input_shape = (1, 3, 640, 640)  # Example input shape\n",
    "# Load the image and ensure it is in RGB mode\n",
    "image = Image.open(\"test.png\").convert(\"RGB\")\n",
    "# Create a preprocessing pipeline; resize to expected dimensions and convert to tensor.\n",
    "preprocess_t = transforms.Compose([\n",
    "    transforms.Resize((input_shape[2], input_shape[3])),\n",
    "    transforms.ToTensor(),\n",
    "    # Optionally add normalization if your model requires it:\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                      std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess_t(image).unsqueeze(0)  # Add batch dimension\n",
    "input_tensor = input_tensor.to(engine.device)\n",
    "input_tensor=preprocess(input_tensor)\n",
    "print(f\"input_tensor shape {input_tensor.shape}\")\n",
    "# Example shapes\n",
    "# input_tensor = torch.randn(1, 3, 640, 640).cuda()\n",
    "orig_img_shape = (1080, 1920)  # Example original image dimensions\n",
    "\n",
    "# Run inference (replace with your TensorRT engine call)\n",
    "output_tensor = engine.infer_async_v3(input_tensor)[0]\n",
    "\n",
    "# Adjust tensor shape\n",
    "output_tensor = output_tensor.permute(0, 2, 1)  # now shape is (1, 8400, 56)\n",
    "\n",
    "# Non-Maximum Suppression (YOLO standard thresholds)\n",
    "preds_nms = non_max_suppression(output_tensor, conf_thres=0.25, iou_thres=0.45)[0]\n",
    "\n",
    "if preds_nms is not None and len(preds_nms):\n",
    "    # Bounding boxes\n",
    "    boxes = preds_nms[:, :4]\n",
    "    boxes = scale_boxes((640, 640), boxes, orig_img_shape)\n",
    "\n",
    "    # Keypoints extraction\n",
    "    keypoints = preds_nms[:, 6:].reshape(-1, 17, 3)\n",
    "    keypoints = scale_coords((640, 640), keypoints, orig_img_shape)\n",
    "\n",
    "    # Output example\n",
    "    for idx, (box, kpts) in enumerate(zip(boxes, keypoints)):\n",
    "        print(f\"Detection {idx}: Box: {box.cpu().numpy()}\")\n",
    "        for kp_idx, (x, y, kp_conf) in enumerate(kpts):\n",
    "            print(f\"\\tKeypoint {kp_idx}: x={x.item():.1f}, y={y.item():.1f}, conf={kp_conf.item():.2f}\")\n",
    "else:\n",
    "    print(\"No detections after NMS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shuttlengine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
